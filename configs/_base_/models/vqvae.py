model = dict(
    type='VQVAE',
    backbone=dict(
        type='SwinTransformer3D',
        arch='tiny',
        pretrained='https://download.openmmlab.com/mmaction/v1.0/recognition/swin/swin_tiny_patch4_window7_224.pth', 
        # pretrained='/home/wangkai/big_space/ant/mmaction2-main/work_dirs/video-swin/best_acc_top1_epoch_44.pth',
        pretrained2d=True,
    patch_size=(2, 4, 4),
        window_size=(8, 7, 7),
        mlp_ratio=4.,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.,
        attn_drop_rate=0.,
        drop_path_rate=0.1,
        patch_norm=True),
    data_preprocessor=dict(
        type='ActionDataPreprocessor',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        format_shape='NCTHW'),
    codebook=dict(
        type='Codebook',
        n_codes=2048,
        embedding_dim=256,
    ),
    decoder=dict(
        type='Decoder',
        n_hiddens=768,
        n_res_layers=4,
        upsample=(2,32,32),
    ),
    pre_vq_conv=dict(
        type='SamePadConv3d',
        in_channels=768,
        out_channels=256,
        kernel_size=1,
        stride=1,
        bias=True,
    ),
    post_vq_conv=dict(
        type='SamePadConv3d',
        in_channels=256,
        out_channels=768,
        kernel_size=1,
        stride=1,
        bias=True,
    ),
)
